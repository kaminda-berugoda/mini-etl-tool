# Mini ETL Framework

A lightweight, modular Python-based ETL pipeline supporting:

- JSON (array) input
- CSV input
- Validation & transformation
- Streaming JSONL output
- Bad record isolation (quarantine)
- Schema drift tracking
- Run reporting
- CLI configuration

This project demonstrates clean data engineering practices including modular design, schema observation, defensive validation, and reproducible execution.

---

## ğŸ— Architecture Overview

Pipeline flow:

Raw Files â†’ Reader â†’ Validator â†’ Transformer â†’ Writer â†’ JSONL Output

Core components:

- `main.py` â€“ Orchestration + CLI
- `config.py` â€“ Configuration model
- `reader.py` â€“ JSON + CSV readers
- `validator.py` â€“ Record validation logic
- `transformer.py` â€“ Canonical transformation logic
- `writer.py` â€“ JSONL writer
- `schema_tracker.py` â€“ Schema observation + drift detection
- `utils.py` â€“ Logging utilities

---

## ğŸ“¦ Supported Input Formats

### JSON (Array Format)

```json
[
  {
    "user_id": "123",
    "name": "John",
    "email": "john@example.com",
    "signup_date": "2026-01-10T12:33:00Z",
    "address": {
      "city": "Sydney",
      "country": "AU"
    },
    "orders": [
      {"order_id": "A1", "amount": "120.50"}
    ]
  }
]
